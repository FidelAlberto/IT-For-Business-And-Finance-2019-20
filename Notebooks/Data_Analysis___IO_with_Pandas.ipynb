{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis - Introduction to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: [Gabriele Pompa](https://www.linkedin.com/in/gabrielepompa/): gabriele.pompa@unisi.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "[Executive Summary](#summary)\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Resources**: \n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary <a name=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the basic imports that we need to work with NumPy, Pandas and to plot data using Matplotlib functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for NumPy arrays\n",
    "import numpy as np\n",
    "\n",
    "# for Pandas Series and DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# for Matplotlib plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to do inline plots in the Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before talking about specific Input/Output (IO) protocols, it is important to mention that typical operating system functionalities (like creating and deleting files, folders, etc) are accessible from Python code using [os module](https://docs.python.org/3/library/os.html). This is a module we will include in our basic imports sectsion hereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use `os.makedirs()` function to create the `Data` folder, under our `IT_For_Business_And_Finance_2019_20` class folder, where we will put all our data files. Function `os.path.exists()` returns `True` if the folder (or file) path it receives in input already exists, otherwise `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolderPath = \"../Data\"\n",
    "\n",
    "if not os.path.exists(dataFolderPath):\n",
    "    os.makedirs(dataFolderPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `..` syntax. The double dots `..` in file path Strings refers to _one directory above_ in the directory tree. Therefore, since the notebook you are reading is located in the `IT_For_Business_And_Finance_2019_20/Notebooks` folder, `../Data` points (and is thus equivalent) to `IT_For_Business_And_Finance_2019_20/Data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IO without Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to IO operations, Python is very flexible and offers several options. We'll review here two typical ways to transfer Python objects across machines:\n",
    "- [JSON](https://docs.python.org/3/library/json.html#module-json) module, which implements human-readable encoding and decoding of basic Python object hierarchies. It is mostly suitable for Python Lists and Dicts.\n",
    "- [Pickle](https://docs.python.org/3/library/pickle.html) module, which implements binary protocols for serializing and de-serializing a Python object structure. It convers a broad spectrum of Python data-structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON format: `json` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[JSON](https://docs.python.org/3/tutorial/inputoutput.html#saving-structured-data-with-json) is the acronym for JavaScript Object Notation. It is a popular data interchange format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `json` Python module can take Python hierarchies (like nested Lists with Dicts inside etc.), _serialize_ them as `.json` files (that is, convert to String representations) and then _deserialize_ them (that is, reconstruct back the original Python object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- JSON format is the standard to send data over a network connection.\n",
    "- `.json` files are, in general, human-readable.\n",
    "\n",
    "Cons: \n",
    "- not all Python objects are serializable using `json` (e.g. NumPy arrays cannot be serialized in this way)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an example. We want to save the `refData` Dict of Python Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S&P Rating': ['A', 'BB', 'AA', 'CCC'],\n",
       " 'Spread': [100, 300, 70, 700],\n",
       " 'Country': ['USA', 'ITA', 'UK', 'ITA']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refData = {\n",
    "    'S&P Rating': ['A', 'BB', 'AA', 'CCC'],\n",
    "    'Spread': [100, 300, 70, 700],\n",
    "    'Country': ['USA', 'ITA', 'UK', 'ITA']\n",
    "}\n",
    "\n",
    "refData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First-of all we import the `json` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the complete file path using the `os.path.join()` function, which concatenates the `dataFolderPath` to `Data` folder, together with `\"refData.json\"`, which is going to be the name of the `.json` file containing the serialized `refData` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = os.path.join(dataFolderPath, \"refData.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create and open a new file `filePath`, we use [`open(filename, mode)` function](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files), giving it the complete path `filePath` to the file to open and mode `'w'` to open it in write-mode. Function `open()` returns a [file-object](https://docs.python.org/3/glossary.html#term-file-object) (which mediates the between IO operations and the underlying resource). We capture it in the `file` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "with open(filePath, 'w') as file:\n",
    "    %time json.dump(refData, file, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here and alsewhere we use the syntax\n",
    "\n",
    "```python\n",
    "%time statement\n",
    "```\n",
    "to execute a statement and measure its execution time (Wall time) with the [`%time` magic function](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function \n",
    "\n",
    "````python\n",
    "json.dump(obj, file_object[, indent])\n",
    "```\n",
    "\n",
    "takes the `refData` object and serializes it as a text file, using the `file_object` file object. The optional argument `indent` is used to pretty-print nested levels of the `refData` object. Here we have used the `\"\\t\"` character so that nested levels are distantiated of one tab. Take a look at `refData.json` file in `Data` folder... you can actually read it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of the [`with` statement](https://www.geeksforgeeks.org/with-statement-in-python/) which:\n",
    "- manages the opening of the file `filePath`, calling `open()` function, \n",
    "- assign the file-object to the `file` variable, through the `as` keyword,\n",
    "- manages the closing of the file after the end of the indented block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have serialized the `refData` object as the `refData.json` file, we can assess whether the file-object is effectively now closed using the `.closed` attribute of the `file` file-object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now reload the serialized object and retrieve the original `refData` object. Same opening through `open()`, but now in reading-mode, using mode `'r'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "with open(filePath, 'r') as file:\n",
    "    %time refData_reloaded = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deserialization (from text file to Python object) is managed by function\n",
    "\n",
    "```python\n",
    "json.load(file_object)\n",
    "```\n",
    "\n",
    "which loads the contents of the file referred by `file_object` and convert them into a Python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S&P Rating': ['A', 'BB', 'AA', 'CCC'],\n",
       " 'Spread': [100, 300, 70, 700],\n",
       " 'Country': ['USA', 'ITA', 'UK', 'ITA']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refData_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finished our IO operation, we can delete our `refData.json` file. We define a utility function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeFile(fileName):\n",
    "    \"\"\"\n",
    "    removeFile(fileName) function remove file 'fileName', if it exists. It also success/failure message on screen.\n",
    "    \n",
    "    Parameters:\n",
    "        fileName (str): name of the file ('Data' folder is assumed)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.isfile(os.path.join(dataFolderPath, fileName)):\n",
    "        os.remove(os.path.join(dataFolderPath, fileName))\n",
    "\n",
    "        # double-check if file still exists\n",
    "        fileStillExists = os.path.isfile(os.path.join(dataFolderPath, fileName))\n",
    "\n",
    "        if fileStillExists:\n",
    "            print(\"Failure: file {} still exists...\".format(fileName))\n",
    "        else:\n",
    "            print(\"Success: file {} successfully removed!\".format(fileName))\n",
    "            \n",
    "    else:\n",
    "        print(\"File {} already removed.\".format(fileName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `os`'s functions:\n",
    "- `os.path.isfile()` which returns `True` if the file in input exists and `False`, otherwise;\n",
    "- `os.remove()` which removes the file in input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: file ../Data\\refData.json successfully removed!\n"
     ]
    }
   ],
   "source": [
    "removeFile(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look in `Data` folder to see that effectively `refData.json` file is not there anymore..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, not all object that you work with in Python are serializable (and thus, transferrable) using the JSON format. A counter-example? NumPy arrays..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unserializableFilePath = os.path.join(dataFolderPath, \"dummyArray.json\")\n",
    "#\n",
    "# with open(unserializableFilePath, 'w') as file:\n",
    "#    \n",
    "#    # raises a TypeError: Object of type ndarray is not JSON serializable\n",
    "#    %time json.dump(np.array([1,2,3]), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pickle` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to JSON, pickle is a protocol which allows the serialization of arbitrarily complex Python objects. In Python, it is implemented in the `pickle` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Pickle works with arbitrary Python obkects (NumPy array and Pandas Series/DataFrames too).\n",
    "\n",
    "Cons: \n",
    "- Pickle format is not cross-platform. That is, a file serialized on a Mac OS might be impossible to de-serialize on a Windows machine (and viceversa).\n",
    "- `.pkl` files are not human-readable.\n",
    "\n",
    "In real life, especially if you have to pass data across different machines, don't use Pickle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an example. We want to save the `mat` NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = int(1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.array([[i*k for i in range(1,rows+1)] for k in range(1,6)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      1,       2,       3,       4,       5],\n",
       "       [      2,       4,       6,       8,      10],\n",
       "       [      3,       6,       9,      12,      15],\n",
       "       ...,\n",
       "       [ 999998, 1999996, 2999994, 3999992, 4999990],\n",
       "       [ 999999, 1999998, 2999997, 3999996, 4999995],\n",
       "       [1000000, 2000000, 3000000, 4000000, 5000000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First-of all we import the `pickle` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.9 ms\n"
     ]
    }
   ],
   "source": [
    "filePath = os.path.join(dataFolderPath, \"mat.pkl\")\n",
    "\n",
    "with open(filePath, 'wb') as file:\n",
    "    %time pickle.dump(mat, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `'wb'` mode when opening the file to store `mat` array. It's going to be a binary file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function \n",
    "\n",
    "````python\n",
    "pickle.dump(obj, file_object)\n",
    "```\n",
    "\n",
    "takes the `mat` object and serializes it as a binary file, using the `file_object` file object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now reload it, using the `'rb'` mode to read the binary file `\"mat.pkl\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.9 ms\n"
     ]
    }
   ],
   "source": [
    "with open(filePath, 'rb') as file:\n",
    "    %time mat_reloaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      1,       2,       3,       4,       5],\n",
       "       [      2,       4,       6,       8,      10],\n",
       "       [      3,       6,       9,      12,      15],\n",
       "       ...,\n",
       "       [ 999998, 1999996, 2999994, 3999992, 4999990],\n",
       "       [ 999999, 1999998, 2999997, 3999996, 4999995],\n",
       "       [1000000, 2000000, 3000000, 4000000, 5000000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean-up Data folder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../Data\\mat.pkl already removed.\n"
     ]
    }
   ],
   "source": [
    "removeFile(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have several object that you want to keep together in a unique file, wrap them in a Python Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_dict = {'mat': mat,\n",
    "            'mat_squared': mat**2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      1,       2,       3,       4,       5],\n",
       "       [      2,       4,       6,       8,      10],\n",
       "       [      3,       6,       9,      12,      15],\n",
       "       ...,\n",
       "       [ 999998, 1999996, 2999994, 3999992, 4999990],\n",
       "       [ 999999, 1999998, 2999997, 3999996, 4999995],\n",
       "       [1000000, 2000000, 3000000, 4000000, 5000000]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_dict['mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[          1,           4,           9,          16,          25],\n",
       "       [          4,          16,          36,          64,         100],\n",
       "       [          9,          36,          81,         144,         225],\n",
       "       ...,\n",
       "       [ -731379964,  1369447440,  2007514916,  1182822464, -1104629916],\n",
       "       [ -729379967,  1377447428,  2025514889,  1214822416, -1054629991],\n",
       "       [ -727379968,  1385447424,  2043514880,  1246822400, -1004630016]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_dict['mat_squared']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = os.path.join(dataFolderPath, \"mat_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51.9 ms\n"
     ]
    }
   ],
   "source": [
    "with open(filePath, 'wb') as file:\n",
    "    %time pickle.dump(mat_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42.9 ms\n"
     ]
    }
   ],
   "source": [
    "with open(filePath, 'rb') as file:\n",
    "    %time mat_dict_reloaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      1,       2,       3,       4,       5],\n",
       "       [      2,       4,       6,       8,      10],\n",
       "       [      3,       6,       9,      12,      15],\n",
       "       ...,\n",
       "       [ 999998, 1999996, 2999994, 3999992, 4999990],\n",
       "       [ 999999, 1999998, 2999997, 3999996, 4999995],\n",
       "       [1000000, 2000000, 3000000, 4000000, 5000000]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_dict_reloaded['mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[          1,           4,           9,          16,          25],\n",
       "       [          4,          16,          36,          64,         100],\n",
       "       [          9,          36,          81,         144,         225],\n",
       "       ...,\n",
       "       [ -731379964,  1369447440,  2007514916,  1182822464, -1104629916],\n",
       "       [ -729379967,  1377447428,  2025514889,  1214822416, -1054629991],\n",
       "       [ -727379968,  1385447424,  2043514880,  1246822400, -1004630016]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_dict_reloaded['mat_squared']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: file ../Data\\mat_dict.pkl successfully removed!\n"
     ]
    }
   ],
   "source": [
    "removeFile(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IO with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas supports IO operations from/to many file formats. As a rule of thumb:\n",
    "\n",
    "- to import data into Pandas, you can use `read_*` functions (like `pd.read_sql()`, `pd.read_csv()`, `pd.read_excel()`, etc.);\n",
    "- to export data from Pandas, you can use `to_*` methods of Pandas DataFrames (like for a `df` DataFrame, `df.to_sql()`, `df.to_csv()`, `df.to_excel()`, etc.);\n",
    "\n",
    "Here we'll review some of the most common file formats:\n",
    "\n",
    "- [SQL](https://it.wikipedia.org/wiki/Structured_Query_Language), using the `sqlite3` module;\n",
    "- [CSV](https://it.wikipedia.org/wiki/Comma-separated_values)\n",
    "- [Excel](https://it.wikipedia.org/wiki/Microsoft_Excel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, SQL stands for Structured Query Language and is a domain-specific language to manage data held in Relational Databases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose we want to save in a SQL table our reference data DataFrame `df_refData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S&amp;P Rating</th>\n",
       "      <th>Spread</th>\n",
       "      <th>Country</th>\n",
       "      <th>Market Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Firm_1</td>\n",
       "      <td>A</td>\n",
       "      <td>100</td>\n",
       "      <td>USA</td>\n",
       "      <td>430.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Firm_2</td>\n",
       "      <td>BB</td>\n",
       "      <td>300</td>\n",
       "      <td>ITA</td>\n",
       "      <td>45.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Firm_3</td>\n",
       "      <td>AA</td>\n",
       "      <td>70</td>\n",
       "      <td>UK</td>\n",
       "      <td>161.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Firm_4</td>\n",
       "      <td>CCC</td>\n",
       "      <td>700</td>\n",
       "      <td>ITA</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       S&P Rating  Spread Country  Market Cap\n",
       "Firm_1          A     100     USA      430.00\n",
       "Firm_2         BB     300     ITA       45.00\n",
       "Firm_3         AA      70      UK      161.25\n",
       "Firm_4        CCC     700     ITA        5.00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_refData = pd.DataFrame(data={\n",
    "                             'S&P Rating': ['A', 'BB', 'AA', 'CCC'],\n",
    "                             'Spread': [100, 300, 70, 700],\n",
    "                             'Country': ['USA', 'ITA', 'UK', 'ITA'],\n",
    "                             'Market Cap': [430.0, 45.0, 161.25, 5.00]\n",
    "                            },\n",
    "                       index=['Firm_1', 'Firm_2', 'Firm_3', 'Firm_4'])\n",
    "\n",
    "df_refData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first follow a step-by-step approach, typing real SQL queries and executing them using the `sqlite3` module. Then we'll explain the real-life approache using `df.to_sql()` method and `pd.read_sql()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL queries from Python: `sqlite3` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLite is library that implements a SQL database engine. The built-in [module `sqlite3`](https://wiki.python.org/moin/SQLite) implements the interface between Python and SQLite, such that you can define SQL tables using Python code and store Pandas DataFrames into them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we import `sqlite3`, giving it the alias `sq3` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sq3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to open a _connection_ to the SQL engine, which creates an empty `refData.db` file in our `Data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = os.path.join(dataFolderPath, \"refData.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sq3.connect(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sq3.connect()` returns a connector `con` that manages the interaction between Python code and SQLite engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlite3.Connection"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the SQL query to create the table `refData`, as a `query` Python string. For details on SQL syntax, there is a good [SQLite tutorial](https://www.sqlitetutorial.net/). Anyway, the meaning of `query` is just creating an empty table `refData` of five columns:\n",
    "- `Firms`: a column of text strings constrained to store non-missing values. We will store here the index of `df_refData`;\n",
    "- `SnP_Rating`: column of text strings that will store the `'S&P Rating'` column;\n",
    "- `Spread`: column of integer numbers that will store the `'Spread'` column;\n",
    "- `Country`: column of text strings that will store the `'Country'` column;\n",
    "- `Market_Cap`: column of float numbers that will store the `'Market Cap'` column;\n",
    "\n",
    "For those who are not familiar with SQL, notice that `TEXT`, `INT` and `REAL` are the `sqlite3` name for `str`, `int` and `float` Python data-types, respectively. For details, see section [SQLite Data Types](https://www.sqlitetutorial.net/sqlite-data-types/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE refData (\n",
      "                Firms TEXT NOT NULL,\n",
      "                SnP_Rating TEXT,\n",
      "                Spread INT,\n",
      "                Country TEXT,\n",
      "                Market_Cap REAL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"CREATE TABLE refData (\n",
    "                Firms TEXT NOT NULL,\n",
    "                SnP_Rating TEXT,\n",
    "                Spread INT,\n",
    "                Country TEXT,\n",
    "                Market_Cap REAL\n",
    ")\"\"\"\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now execute the query using the `.execute()` method of the connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1ef43b97110>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `.commit()` method, we actually implement the changes due to the run of the `query` to the `\"refData.db\"` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can actually open the `\"refData.db\"` using [DB Browser for SQLite](https://sqlitebrowser.org/) and there you'll see an empty table `refData` of five columns and column types as requested (under 'Browse Data' tab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store `df_refData` into `refData` table row-by-row, using the `.iterrows()` method, which in a for loop returns the index and a Pandas Series of the given row. Check it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: Firm_1 \n",
      "Row: \n",
      "S&P Rating      A\n",
      "Spread        100\n",
      "Country       USA\n",
      "Market Cap    430\n",
      "Name: Firm_1, dtype: object \n",
      "\n",
      "\n",
      "Index: Firm_2 \n",
      "Row: \n",
      "S&P Rating     BB\n",
      "Spread        300\n",
      "Country       ITA\n",
      "Market Cap     45\n",
      "Name: Firm_2, dtype: object \n",
      "\n",
      "\n",
      "Index: Firm_3 \n",
      "Row: \n",
      "S&P Rating        AA\n",
      "Spread            70\n",
      "Country           UK\n",
      "Market Cap    161.25\n",
      "Name: Firm_3, dtype: object \n",
      "\n",
      "\n",
      "Index: Firm_4 \n",
      "Row: \n",
      "S&P Rating    CCC\n",
      "Spread        700\n",
      "Country       ITA\n",
      "Market Cap      5\n",
      "Name: Firm_4, dtype: object \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_refData.iterrows():\n",
    "    print(\"Index: {} \\nRow: \\n{} \\n\\n\".format(index, row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `INSERT INTO refData VALUES...` query is responsible to store the values of each row. We write and `.execute()` one query per row, replacing the `{}` brackets in the string declaration with the appropriate values of each column of the given `row`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO refData VALUES ('Firm_1', 'A', 100, 'USA', 430.0)\n",
      "INSERT INTO refData VALUES ('Firm_2', 'BB', 300, 'ITA', 45.0)\n",
      "INSERT INTO refData VALUES ('Firm_3', 'AA', 70, 'UK', 161.25)\n",
      "INSERT INTO refData VALUES ('Firm_4', 'CCC', 700, 'ITA', 5.0)\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_refData.iterrows():\n",
    "    query = \"INSERT INTO refData VALUES ('{}', '{}', {}, '{}', {})\".\\\n",
    "    format(index, row['S&P Rating'], row['Spread'], row[\"Country\"], row[\"Market Cap\"])\n",
    "    \n",
    "    print(query)\n",
    "    con.execute(query)\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all the `df_refData.iterrows()` is over we `.commit()` the changes. Check it out with DB Browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have stored values into the `refData` table, we can retrieve them using the standard `SELECT *` query. Rember always to commit changes after you have exectud your query, otherwise nothing will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1ef43b97b20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"SELECT * FROM refData\"\n",
    "cursor = con.execute(query)\n",
    "con.commit()\n",
    "cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we capture the output of `.execute()` in a `cursor`, which we can use to fetch the data as per the `SELECT *` query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Firm_1', 'A', 100, 'USA', 430.0),\n",
       " ('Firm_2', 'BB', 300, 'ITA', 45.0),\n",
       " ('Firm_3', 'AA', 70, 'UK', 161.25),\n",
       " ('Firm_4', 'CCC', 700, 'ITA', 5.0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=cursor.fetchall()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the values are there, but the `data` returned are not the original `df_refData` Pandas DataFrame, but in the form of a Python List. With some List comprehension effort, we can reconstruct our original DataFrame... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S&amp;P Rating</th>\n",
       "      <th>Spread</th>\n",
       "      <th>Country</th>\n",
       "      <th>Market Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Firm_1</td>\n",
       "      <td>A</td>\n",
       "      <td>100</td>\n",
       "      <td>USA</td>\n",
       "      <td>430.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Firm_2</td>\n",
       "      <td>BB</td>\n",
       "      <td>300</td>\n",
       "      <td>ITA</td>\n",
       "      <td>45.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Firm_3</td>\n",
       "      <td>AA</td>\n",
       "      <td>70</td>\n",
       "      <td>UK</td>\n",
       "      <td>161.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Firm_4</td>\n",
       "      <td>CCC</td>\n",
       "      <td>700</td>\n",
       "      <td>ITA</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       S&P Rating  Spread Country  Market Cap\n",
       "Firm_1          A     100     USA      430.00\n",
       "Firm_2         BB     300     ITA       45.00\n",
       "Firm_3         AA      70      UK      161.25\n",
       "Firm_4        CCC     700     ITA        5.00"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_refData_reloaded = pd.DataFrame(data=[t[1:] for t in data],\n",
    "                                   index=[t[0] for t in data],\n",
    "                                   columns=['S&P Rating', 'Spread', 'Country', 'Market Cap'])\n",
    "\n",
    "df_refData_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we'll see that Pandas provides much more efficient methods to save and retrieve data with and SQL engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you stop working with a SQL engine, close the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean-up our `Data` folder deleting the `\"refData.db\"` file (if still open, shut down DB Browser otherwise you won't be able to remove the file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../Data\\refData.db already removed.\n"
     ]
    }
   ],
   "source": [
    "removeFile(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas and SQL: `df.to_sql()` and `pd.read_sql()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM refData\"\n",
    "\n",
    "df_refData_reloaded = pd.read_sql(sql=query, con=con)\n",
    "\n",
    "df_refData_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT * FROM refData\"\"\"\n",
    "\n",
    "df_refData_reloaded = pd.read_sql(sql=query, con=con, index_col=\"Firms\")\n",
    "\n",
    "df_refData_reloaded = df_refData_reloaded.rename(columns={old_col: new_col for old_col, new_col \n",
    "                                                          in zip(df_refData_reloaded.columns, df_refData.columns)})\n",
    "df_refData_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM refData WHERE Market_Cap > 100\"\n",
    "\n",
    "pd.read_sql(sql=query, con=con, index_col=\"Firms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData_reloaded[\"Market Cap\"][df_refData_reloaded[\"Market Cap\"] > 100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData.to_sql(name=\"refData\", con=con, index_label=\"Firms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM refData\"\n",
    "\n",
    "df_refData_reloaded = pd.read_sql(sql=query, con=con, index_col=\"Firms\")\n",
    "\n",
    "df_refData_reloaded = df_refData_reloaded.rename(columns={old_col: new_col for old_col, new_col \n",
    "                                                          in zip(df_refData_reloaded.columns, df_refData.columns)})\n",
    "df_refData_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### another example - parsing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=np.array([[i**k for i in range(1,11)] for k in range(1,6)]).T, \n",
    "                  index=pd.date_range('2020-01-01', periods=10, freq='B'), \n",
    "                  columns=['x', 'x^2', 'x^3', 'x^4', 'x^5'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table\n",
    "con = sq3.connect(dataFolderPath + \"df.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(name=\"df\", con=con, index_label=\"Dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM df\"\n",
    "df_reloaded = pd.read_sql(sql=query, con=con, index_col=\"Dates\")\n",
    "df_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_reloaded.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM df\"\n",
    "df_reloaded = pd.read_sql(sql=query, con=con, index_col=\"Dates\", parse_dates=\"Dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PANDAS + .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_refData.to_csv(path_or_buf = dataFolderPath + \"df_refData.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_refData_reloaded = pd.read_csv(filepath_or_buffer = dataFolderPath + \"df_refData.json\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_refData_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(dataFolderPath + \"df_refData.json\"):\n",
    "    os.remove(dataFolderPath + \"df_refData.json\")\n",
    "\n",
    "# double-check if file still exists\n",
    "os.path.isfile(dataFolderPath + \"df_refData.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### another example - parsing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.to_csv(path_or_buf = dataFolderPath + \"df.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_reloaded = pd.read_csv(filepath_or_buffer = dataFolderPath + \"df.json\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_reloaded.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_reloaded = pd.read_csv(filepath_or_buffer = dataFolderPath + \"df.json\", index_col = 0, parse_dates = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(dataFolderPath + \"df.json\"):\n",
    "    os.remove(dataFolderPath + \"df.json\")\n",
    "\n",
    "# double-check if file still exists\n",
    "os.path.isfile(dataFolderPath + \"df.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PANDAS + Excel (FORSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_refData.to_excel(excel_writer = dataFolderPath + \"df_refData.xlsx\", sheet_name = \"reference data table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_refData_reloaded = pd.read_excel(io = dataFolderPath + \"df_refData.xlsx\", index_col = 0, sheet_name = \"reference data table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_refData_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(dataFolderPath + \"df_refData.xlsx\"):\n",
    "    os.remove(dataFolderPath + \"df_refData.xlsx\")\n",
    "\n",
    "# double-check if file still exists\n",
    "os.path.isfile(dataFolderPath + \"df_refData.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### another example - parsing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.to_excel(excel_writer = dataFolderPath + \"df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_reloaded = pd.read_excel(io = dataFolderPath + \"df.xlsx\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(dataFolderPath + \"df.xlsx\"):\n",
    "    os.remove(dataFolderPath + \"df.xlsx\")\n",
    "\n",
    "# double-check if file still exists\n",
    "os.path.isfile(dataFolderPath + \"df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PANDAS + Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Yahoo Finance API\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = yf.download(\"^GSPC\", period=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2010-01-01':, 'High'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spx = yf.Ticker(\"^GSPC\")\n",
    "spx_hist = spx.history(period=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by = 'ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['SPY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
