{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis - Introduction to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: [Gabriele Pompa](https://www.linkedin.com/in/gabrielepompa/): gabriele.pompa@unisi.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "[Executive Summary](#summary)\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Resources**: \n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary <a name=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the basic imports that we need to work with NumPy, Pandas and to plot data using Matplotlib functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for NumPy arrays\n",
    "import numpy as np\n",
    "\n",
    "# for Pandas Series and DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# for Matplotlib plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to do inline plots in the Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input/Output in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before talking about specific Input/Output (IO) protocols, it is important to mention that typical operating system functionalities (like creating and deleting files, folders, etc) are accessible from Python code using [os module](https://docs.python.org/3/library/os.html). This is a module we will include in our basic imports sectsion hereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use `os.makedirs()` function to create the `Data` folder, under our `IT_For_Business_And_Finance_2019_20` class folder, where we will put all our data files. Function `os.path.exists()` returns `True` if the folder (or file) path it receives in input already exists, otherwise `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolderPath = \"../Data\"\n",
    "\n",
    "if not os.path.exists(dataFolderPath):\n",
    "    os.makedirs(dataFolderPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `..` syntax. The double dots `..` in file path Strings refers to _one directory above_ in the directory tree. Therefore, since the notebook you are reading is located in the `IT_For_Business_And_Finance_2019_20/Notebooks` folder, `../Data` points (and is thus equivalent) to `IT_For_Business_And_Finance_2019_20/Data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IO without Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to IO operations, Python is very flexible and offers several options. We'll review here two typical ways to transfer Python objects across machines:\n",
    "- [JSON](https://docs.python.org/3/library/json.html#module-json) module, which implements human-readable encoding and decoding of basic Python object hierarchies. It is mostly suitable for Python Lists and Dicts.\n",
    "- [Pickle](https://docs.python.org/3/library/pickle.html) module, which implements binary protocols for serializing and de-serializing a Python object structure. It convers a broad spectrum of Python data-structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON format: `json` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[JSON](https://docs.python.org/3/tutorial/inputoutput.html#saving-structured-data-with-json) is the acronym for JavaScript Object Notation. It is a popular data interchange format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `json` Python module can take Python hierarchies (like nested Lists with Dicts inside etc.), _serialize_ them as `.json` files (that is, convert to String representations) and then _deserialize_ them (that is, reconstruct back the original Python object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- JSON format is the standard to send data over a network connection.\n",
    "- `.json` files are, in general, human-readable.\n",
    "\n",
    "Cons: \n",
    "- not all Python objects are serializable using `json` (e.g. NumPy arrays cannot be serialized in this way)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an example. We want to save the `refData` Dict of Python Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S&P Rating': ['A', 'BB', 'AA', 'CCC'],\n",
       " 'Spread': [100, 300, 70, 700],\n",
       " 'Country': ['USA', 'ITA', 'UK', 'ITA']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refData = {\n",
    "    'S&P Rating': ['A', 'BB', 'AA', 'CCC'],\n",
    "    'Spread': [100, 300, 70, 700],\n",
    "    'Country': ['USA', 'ITA', 'UK', 'ITA']\n",
    "}\n",
    "\n",
    "refData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First-of all we import the `json` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the complete file path using the `os.path.join()` function, which concatenates the `dataFolderPath` to `Data` folder, together with `\"refData.json\"`, which is going to be the name of the `.json` file containing the serialized `refData` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = os.path.join(dataFolderPath, \"refData.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create and open a new file `filePath`, we use [`open(filename, mode)` function](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files), giving it the complete path `filePath` to the file to open and mode `'w'` to open it in write-mode. Function `open()` returns a [file-object](https://docs.python.org/3/glossary.html#term-file-object) (which mediates the between IO operations and the underlying resource). We capture it in the `file` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "with open(filePath, 'w') as file:\n",
    "    %time json.dump(refData, file, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here and alsewhere we use the syntax\n",
    "\n",
    "```python\n",
    "%time statement\n",
    "```\n",
    "to execute a statement and measure its execution time (Wall time) with the [`%time` magic function](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function \n",
    "\n",
    "````python\n",
    "json.dump(obj, file_object[, indent])\n",
    "```\n",
    "\n",
    "takes the `refData` object and serializes it as a text file, using the `file_object` file object. The optional argument `indent` is used to pretty-print nested levels of the `refData` object. Here we have used the `\"\\t\"` character so that nested levels are distantiated of one tab. Take a look at `refData.json` file in `Data` folder... you can actually read it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of the [`with` statement](https://www.geeksforgeeks.org/with-statement-in-python/) which:\n",
    "- manages the opening of the file `filePath`, calling `open()` function, \n",
    "- assign the file-object to the `file` variable, through the `as` keyword,\n",
    "- manages the closing of the file after the end of the indented block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have serialized the `refData` object as the `refData.json` file, we can assess whether the file-object is effectively now closed using the `.closed` attribute of the `file` file-object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now reload the serialized object and retrieve the original `refData` object. Same opening through `open()`, but now in reading-mode, using mode `'r'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "with open(filePath, 'r') as file:\n",
    "    %time refData_reloaded = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deserialization (from text file to Python object) is managed by function\n",
    "\n",
    "```python\n",
    "json.load(file_object)\n",
    "```\n",
    "\n",
    "which loads the contents of the file referred by `file_object` and convert them into a Python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S&P Rating': ['A', 'BB', 'AA', 'CCC'],\n",
       " 'Spread': [100, 300, 70, 700],\n",
       " 'Country': ['USA', 'ITA', 'UK', 'ITA']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refData_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finished our IO operation, we can delete our `refData.json` file. We define a utility function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeFile(fileName):\n",
    "    \"\"\"\n",
    "    removeFile(fileName) function remove file 'fileName', if it exists. It also success/failure message on screen.\n",
    "    \n",
    "    Parameters:\n",
    "        fileName (str): name of the file ('Data' folder is assumed)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.isfile(os.path.join(dataFolderPath, fileName)):\n",
    "        os.remove(os.path.join(dataFolderPath, fileName))\n",
    "\n",
    "        # double-check if file still exists\n",
    "        fileStillExists = os.path.isfile(os.path.join(dataFolderPath, fileName))\n",
    "\n",
    "        if fileStillExists:\n",
    "            print(\"Failure: file {} still exists...\".format(fileName))\n",
    "        else:\n",
    "            print(\"Success: file {} successfully removed!\".format(fileName))\n",
    "            \n",
    "    else:\n",
    "        print(\"File {} already removed.\".format(fileName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `os`'s functions:\n",
    "- `os.path.isfile()` which returns `True` if the file in input exists and `False`, otherwise;\n",
    "- `os.remove()` which removes the file in input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: file ../Data\\refData.json successfully removed!\n"
     ]
    }
   ],
   "source": [
    "removeFile(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look in `Data` folder to see that effectively `refData.json` file is not there anymore..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, not all object that you work with in Python are serializable (and thus, transferrable) using the JSON format. A counter-example? NumPy arrays..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unserializableFilePath = os.path.join(dataFolderPath, \"dummyArray.json\")\n",
    "#\n",
    "# with open(unserializableFilePath, 'w') as file:\n",
    "#    \n",
    "#    # raises a TypeError: Object of type ndarray is not JSON serializable\n",
    "#    %time json.dump(np.array([1,2,3]), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pickle` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to JSON, pickle is a protocol which allows the serialization of arbitrarily complex Python objects. In Python, it is implemented in the `pickle` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Pickle works with arbitrary Python obkects (NumPy array and Pandas Series/DataFrames too).\n",
    "\n",
    "Cons: \n",
    "- Pickle format is not cross-platform. That is, a file serialized on a Mac OS might be impossible to de-serialize on a Windows machine (and viceversa).\n",
    "- `.pkl` files are not human-readable.\n",
    "\n",
    "In real life, especially if you have to pass data across different machines, don't use Pickle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an example. We want to save the `mat` NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = int(1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.array([[i*k for i in range(1,rows+1)] for k in range(1,6)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      1,       2,       3,       4,       5],\n",
       "       [      2,       4,       6,       8,      10],\n",
       "       [      3,       6,       9,      12,      15],\n",
       "       ...,\n",
       "       [ 999998, 1999996, 2999994, 3999992, 4999990],\n",
       "       [ 999999, 1999998, 2999997, 3999996, 4999995],\n",
       "       [1000000, 2000000, 3000000, 4000000, 5000000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First-of all we import the `pickle` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.9 ms\n"
     ]
    }
   ],
   "source": [
    "filePath = os.path.join(dataFolderPath, \"mat.pkl\")\n",
    "\n",
    "with open(filePath, 'wb') as file:\n",
    "    %time pickle.dump(mat, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `'wb'` mode when opening the file to store `mat` array. It's going to be a binary file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function \n",
    "\n",
    "````python\n",
    "pickle.dump(obj, file_object)\n",
    "```\n",
    "\n",
    "takes the `mat` object and serializes it as a binary file, using the `file_object` file object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now reload it, using the `'rb'` mode to read the binary file `\"mat.pkl\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.9 ms\n"
     ]
    }
   ],
   "source": [
    "with open(filePath, 'rb') as file:\n",
    "    %time mat_reloaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      1,       2,       3,       4,       5],\n",
       "       [      2,       4,       6,       8,      10],\n",
       "       [      3,       6,       9,      12,      15],\n",
       "       ...,\n",
       "       [ 999998, 1999996, 2999994, 3999992, 4999990],\n",
       "       [ 999999, 1999998, 2999997, 3999996, 4999995],\n",
       "       [1000000, 2000000, 3000000, 4000000, 5000000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean-up Data folder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../Data\\mat.pkl already removed.\n"
     ]
    }
   ],
   "source": [
    "removeFile(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have several object that you want to keep together in a unique file, wrap them in a Python Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_dict = {'mat': mat,\n",
    "            'mat_squared': mat**2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      1,       2,       3,       4,       5],\n",
       "       [      2,       4,       6,       8,      10],\n",
       "       [      3,       6,       9,      12,      15],\n",
       "       ...,\n",
       "       [ 999998, 1999996, 2999994, 3999992, 4999990],\n",
       "       [ 999999, 1999998, 2999997, 3999996, 4999995],\n",
       "       [1000000, 2000000, 3000000, 4000000, 5000000]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_dict['mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[          1,           4,           9,          16,          25],\n",
       "       [          4,          16,          36,          64,         100],\n",
       "       [          9,          36,          81,         144,         225],\n",
       "       ...,\n",
       "       [ -731379964,  1369447440,  2007514916,  1182822464, -1104629916],\n",
       "       [ -729379967,  1377447428,  2025514889,  1214822416, -1054629991],\n",
       "       [ -727379968,  1385447424,  2043514880,  1246822400, -1004630016]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_dict['mat_squared']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = os.path.join(dataFolderPath, \"mat_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51.9 ms\n"
     ]
    }
   ],
   "source": [
    "with open(filePath, 'wb') as file:\n",
    "    %time pickle.dump(mat_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42.9 ms\n"
     ]
    }
   ],
   "source": [
    "with open(filePath, 'rb') as file:\n",
    "    %time mat_dict_reloaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      1,       2,       3,       4,       5],\n",
       "       [      2,       4,       6,       8,      10],\n",
       "       [      3,       6,       9,      12,      15],\n",
       "       ...,\n",
       "       [ 999998, 1999996, 2999994, 3999992, 4999990],\n",
       "       [ 999999, 1999998, 2999997, 3999996, 4999995],\n",
       "       [1000000, 2000000, 3000000, 4000000, 5000000]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_dict_reloaded['mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[          1,           4,           9,          16,          25],\n",
       "       [          4,          16,          36,          64,         100],\n",
       "       [          9,          36,          81,         144,         225],\n",
       "       ...,\n",
       "       [ -731379964,  1369447440,  2007514916,  1182822464, -1104629916],\n",
       "       [ -729379967,  1377447428,  2025514889,  1214822416, -1054629991],\n",
       "       [ -727379968,  1385447424,  2043514880,  1246822400, -1004630016]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_dict_reloaded['mat_squared']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: file ../Data\\mat_dict.pkl successfully removed!\n"
     ]
    }
   ],
   "source": [
    "removeFile(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IO with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "\n",
    "When it comes to IO operations, Python is very flexible and offers several options. We'll review here two typical ways to transfer Python objects across machines:\n",
    "- [JSON](https://docs.python.org/3/library/json.html#module-json) module, which implements human-readable encoding and decoding of basic Python object hierarchies. It is mostly suitable for Python Lists and Dicts.\n",
    "- [Pickle](https://docs.python.org/3/library/pickle.html) module, which implements binary protocols for serializing and de-serializing a Python object structure. It convers a broad spectrum of Python data-structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData = pd.DataFrame(data={\n",
    "                             'S&P Rating': ['A', 'BB', 'AA', 'CCC'],\n",
    "                             'Spread': [100, 300, 70, 700],\n",
    "                             'Country': ['USA', 'ITA', 'UK', 'ITA'],\n",
    "                             'Market Cap': [430.0, 45.0, 161.25, 5.00]\n",
    "                            },\n",
    "                       index=['Firm_1', 'Firm_2', 'Firm_3', 'Firm_4'])\n",
    "\n",
    "df_refData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PANDAS + SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sq3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table\n",
    "con = sq3.connect(dataFolderPath + \"refData.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"CREATE TABLE refData (\n",
    "                Firms TEXT NOT NULL,\n",
    "                SnP_Rating TEXT,\n",
    "                Spread INT,\n",
    "                Country TEXT,\n",
    "                Market_Cap REAL\n",
    ")\"\"\"\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_refData.iterrows():\n",
    "    print(index, row, row.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_refData.iterrows():\n",
    "    print(index, row['S&P Rating'], row['Spread'], row[\"Country\"], row[\"Market Cap\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_refData.iterrows():\n",
    "    query = \"INSERT INTO refData VALUES ('{}', '{}', {}, '{}', {})\".\\\n",
    "    format(index, row['S&P Rating'], row['Spread'], row[\"Country\"], row[\"Market Cap\"])\n",
    "    \n",
    "    print(query)\n",
    "    con.execute(query)\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting using [DB Browser for SQLite](https://sqlitebrowser.org/dl/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM refData\"\n",
    "cursor = con.execute(query)\n",
    "con.commit()\n",
    "cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=cursor.fetchall()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded = pd.DataFrame(data=[t[1:] for t in data],\n",
    "                           index=[t[0] for t in data],\n",
    "                           columns=['S&P Rating', 'Spread', 'Country', 'Market Cap'])\n",
    "\n",
    "df_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM refData\"\n",
    "\n",
    "df_refData_reloaded = pd.read_sql(sql=query, con=con)\n",
    "\n",
    "df_refData_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT * FROM refData\"\"\"\n",
    "\n",
    "df_refData_reloaded = pd.read_sql(sql=query, con=con, index_col=\"Firms\")\n",
    "\n",
    "df_refData_reloaded = df_refData_reloaded.rename(columns={old_col: new_col for old_col, new_col \n",
    "                                                          in zip(df_refData_reloaded.columns, df_refData.columns)})\n",
    "df_refData_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM refData WHERE Market_Cap > 100\"\n",
    "\n",
    "pd.read_sql(sql=query, con=con, index_col=\"Firms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData_reloaded[\"Market Cap\"][df_refData_reloaded[\"Market Cap\"] > 100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData.to_sql(name=\"refData\", con=con, index_label=\"Firms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM refData\"\n",
    "\n",
    "df_refData_reloaded = pd.read_sql(sql=query, con=con, index_col=\"Firms\")\n",
    "\n",
    "df_refData_reloaded = df_refData_reloaded.rename(columns={old_col: new_col for old_col, new_col \n",
    "                                                          in zip(df_refData_reloaded.columns, df_refData.columns)})\n",
    "df_refData_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### another example - parsing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=np.array([[i**k for i in range(1,11)] for k in range(1,6)]).T, \n",
    "                  index=pd.date_range('2020-01-01', periods=10, freq='B'), \n",
    "                  columns=['x', 'x^2', 'x^3', 'x^4', 'x^5'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table\n",
    "con = sq3.connect(dataFolderPath + \"df.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(name=\"df\", con=con, index_label=\"Dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM df\"\n",
    "df_reloaded = pd.read_sql(sql=query, con=con, index_col=\"Dates\")\n",
    "df_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_reloaded.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM df\"\n",
    "df_reloaded = pd.read_sql(sql=query, con=con, index_col=\"Dates\", parse_dates=\"Dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PANDAS + .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_refData.to_csv(path_or_buf = dataFolderPath + \"df_refData.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_refData_reloaded = pd.read_csv(filepath_or_buffer = dataFolderPath + \"df_refData.json\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_refData_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(dataFolderPath + \"df_refData.json\"):\n",
    "    os.remove(dataFolderPath + \"df_refData.json\")\n",
    "\n",
    "# double-check if file still exists\n",
    "os.path.isfile(dataFolderPath + \"df_refData.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### another example - parsing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.to_csv(path_or_buf = dataFolderPath + \"df.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_reloaded = pd.read_csv(filepath_or_buffer = dataFolderPath + \"df.json\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_reloaded.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_reloaded = pd.read_csv(filepath_or_buffer = dataFolderPath + \"df.json\", index_col = 0, parse_dates = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(dataFolderPath + \"df.json\"):\n",
    "    os.remove(dataFolderPath + \"df.json\")\n",
    "\n",
    "# double-check if file still exists\n",
    "os.path.isfile(dataFolderPath + \"df.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PANDAS + Excel (FORSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_refData.to_excel(excel_writer = dataFolderPath + \"df_refData.xlsx\", sheet_name = \"reference data table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_refData_reloaded = pd.read_excel(io = dataFolderPath + \"df_refData.xlsx\", index_col = 0, sheet_name = \"reference data table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_refData_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(dataFolderPath + \"df_refData.xlsx\"):\n",
    "    os.remove(dataFolderPath + \"df_refData.xlsx\")\n",
    "\n",
    "# double-check if file still exists\n",
    "os.path.isfile(dataFolderPath + \"df_refData.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### another example - parsing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.to_excel(excel_writer = dataFolderPath + \"df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_reloaded = pd.read_excel(io = dataFolderPath + \"df.xlsx\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reloaded.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(dataFolderPath + \"df.xlsx\"):\n",
    "    os.remove(dataFolderPath + \"df.xlsx\")\n",
    "\n",
    "# double-check if file still exists\n",
    "os.path.isfile(dataFolderPath + \"df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PANDAS + Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Yahoo Finance API\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = yf.download(\"^GSPC\", period=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2010-01-01':, 'High'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spx = yf.Ticker(\"^GSPC\")\n",
    "spx_hist = spx.history(period=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by = 'ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['SPY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
